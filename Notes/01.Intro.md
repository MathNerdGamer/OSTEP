# Introduction to Operating Systems

* When a program runs, it does three things (ignoring a bunch of extra stuff done for performance):
    1. It *fetches* an instruction from memory.
    2. It *decodes* the instruction.
    3. It *executes* the instruction.
    4. Then, it starts from the beginning for the next instruction.
    * This is known as the *von Neumann* model of computing, named after John von Neumann.

* An *operating system* is essentially software that makes it easy to run programs.

* An operating system does this is by employing a technique known as *virtualization*.
    * Virtualization is taking a physical resource and abstracting away the implementation details to its most fundamental form.
    * As a result, operating systems are sometimes called *virtual machines* (as opposed to something like VirtualBox, which is technically a *virtual machine monitor*, or *hypervisor*).
    * Figuring out how an operating system might do this is one of the main goals of this course.

* In order for users to make use of the OS, *interfaces* (aka *APIs*) are provided in the form of a *system call* (*syscall*).
    * A few hundred syscalls may be provided by an OS.
    * These syscalls allow for practically all of the things an OS is made for, such as running programs, accessing memory or devices, or anything else that needs the OS to provide that layer of abstraction for access to physical hardware.
    * As a result, an operating system is said to provide a *standard library* for such actions.

* Since virtualization allows for shared use of system resources, an OS is sometimes referred to ask a *resource manager*.
    * Resources include, but are not limited to, CPU time for running a program (or, more commonly, **many** programs concurrently) and access to hardware attached to the system in the form of persistent storage, extra compute units (i.e., a GPU), or networking.
    * The operating system, in an attempt to keep some semblance of order in all of the chaos, has to manage these resources carefully and efficiently.

## Virtualizing the CPU

File: `cpu.c`
```c
#include <stdio.h>
#include <stdlib.h>
#include "common.h"

int main(int argc, char *argv[])
{
    if (argc != 2) {
	fprintf(stderr, "usage: cpu <string>\n");
	exit(1);
    }
    char *str = argv[1];

    while (1) {
	printf("%s\n", str);
	Spin(1);
    }
    return 0;
}
```

* The above code is our first example.
    * All it does is loop forever printing a string out and calling `Spin()`.
    * What does `Spin()` even do?

File: `common.h`
```c
#ifndef __common_h__
#define __common_h__

#include <sys/time.h>
#include <sys/stat.h>
#include <assert.h>

double GetTime() {
    struct timeval t;
    int rc = gettimeofday(&t, NULL);
    assert(rc == 0);
    return (double) t.tv_sec + (double) t.tv_usec/1e6;
}

void Spin(int howlong) {
    double t = GetTime();
    while ((GetTime() - t) < (double) howlong)
	; // do nothing in loop
}

#endif // __common_h__
```

* Taking a look at the `common.h` header file, it seems `Spin()` takes an integer and, for the number of seconds specified by that integer, does nothing.

* When we run `./cpu A`, we get the following output:
    ```
    shell> ./cpu A
    A
    A
    A
    A
    ^C
    shell>
    ```
    * Here, we used `CTRL+C` to end execution.

* What about if we use `&` to start multiple versions of `./cpu` with different arguments?
    ```
    shell> ./cpu A & ./cpu B & ./cpu C & ./cpu D &
    [1] 14570
    [2] 14571
    [3] 14572
    [4] 14573
    A
    B
    C
    D
    A
    B
    C
    D
    . . .
    kill 14570 & kill 14571 & kill 14572 & kill 14573
    ```
    * Make note of the *pid* of each command so that they can be killed, or else it'll just go on forever.
    * `CTRL+C` won't work here.

* Note that we get multiple versions of our command running concurrently.
    * How the operating system decides how and when each command gets turn executing will be covered later.
    * Such decision-making is known as a *policy*.
    * It's through the policy specified by the operating system that multiple programs can share the CPU to execute.

* Allowing for multiple programs to run at seemingly the same time is an example of the operating system *virtualizing the CPU*.

## Virtualizing Memory

* Operating systems also *virtualize memory*.
    * Operating systems specify a simplistic model for *physical memory*:
        * Memory is an array of bytes.
        * To *read* from memory, one must specify an *address*.
        * To *write* (or *update*) memory, one must specify both an address and the data to be written.

* Recall that, in the von Neumann model, both the data we're operating on **and** the instructions of our program are in memory.

File: `mem.c`
```c
#include <unistd.h>
#include <stdio.h>
#include <stdlib.h>
#include "common.h"

int main(int argc, char *argv[]) {
    if (argc != 2) { 
	    fprintf(stderr, "usage: mem <value>\n"); 
	    exit(1); 
    } 
    int *p; 
    p = malloc(sizeof(int));
    assert(p != NULL);
    printf("(%d) addr pointed to by p: %p\n", (int) getpid(), p);
    *p = atoi(argv[1]); // assign value to addr stored in p
    while (1) {
	    Spin(1);
	    *p = *p + 1;
	    printf("(%d) value of p: %d\n", getpid(), *p);
    }
    return 0;
}
```

* Looking at `mem.c`:
    * We allocate an `int` on the heap and assign the address to pointer `p`.
    * In an infinite loop, we spin for a second, increment the integer pointed to by `p`, and print it out.

* Below is an example output.
    ```
    shell> ./mem 0
    (20119) addr pointed to by p: 0x5555555592a0
    (20119) value of p: 1
    (20119) value of p: 2
    (20119) value of p: 3
    (20119) value of p: 4
    (20119) value of p: 5
    (20119) value of p: 6
    (20119) value of p: 7
    (20119) value of p: 8
    (20119) value of p: 9
    ^C
    shell>
    ```

* If we run multiple instances of this, we'll notice something interesting.
    ```
    shell> ./mem 0 & ./mem 10 &
    [1] 21459
    [2] 21460
    (21459) addr pointed to by p: 0x5555555592a0
    (21460) addr pointed to by p: 0x5555555592a0
    (21459) value of p: 1
    (21460) value of p: 11
    (21459) value of p: 2
    (21460) value of p: 12
    (21459) value of p: 3
    (21460) value of p: 13
    (21459) value of p: 4
    (21460) value of p: 14
    . . .
    kill 21459 & kill 21460
    ```

* Notice that the addresses were the same, even though the programs clearly see different values at that address.
    * This is a result of memory virtualization.
    * Each program gets its own private *virtual memory address space* (or just *address space*).
    * They are isolated in their own chunks of memory, and they can run as though they have all of physical memory without worrying about stepping on another program's memory.

* The above examples were run with a security feature, *Address Space Layout Randomization* (*ASLR*), disabled.
    * To emulate this, run `echo 0 | sudo tee /proc/sys/kernel/randomize_va_space`.
    * However, be *absolutely sure* that you re-enable full randomization afterwards using `echo 2 | sudo tee /proc/sys/kernel/randomize_va_space`.
    * If we were to run without disabling ASLR, the addresses would be different, which wouldn't demonstrate memory virtualization very well.
    * There are legitimate security reasons for why, despite each program having their own address spaces, it's still necessary to have them point to random (and probably different) addresses as the start of their stacks.
        * See: [Stack Buffer Overflow](https://en.wikipedia.org/wiki/Stack_buffer_overflow)

## Concurrency

* *Concurrency* is the ability of programs to execute instructions in segments where we may interrupt one segment to allow for the execution of a different segment.
    * For example, we may have two tasks that might have some level of inter-dependence, but can be executed in such a way that, when one task has to wait (such as for some I/O operation), the other task can start from where it previously stopped.
    * This allows for less time spent with the CPU doing nothing -- or worse, spending cycles, and the accompanying electricity, just to wait.

* The first examples of concurrency come from operating systems, themselves.
    * Recall that we've been running multiple versions of the example code above where the execution seemingly overlapped.
    * This is a result of the operating system using concurrency to make efficient use of the CPU to stop working on one program when it runs into the `Spin()` function to work on the next.

* The problems that come with concurrency can be quite difficult to wrap one's head around and, nowadays, are involved in more than just the operating system.
    * Many programs are now *multi-threaded*.
    * We see an example of a multi-threaded program below, along with a problem that arises due to a bug that might not be obvious on first inspection, especially when first seeing threads.

File: `threads.c`
```c
#include <stdio.h>
#include <stdlib.h>
#include "common.h"
#include "common_threads.h"

volatile int counter = 0; 
int loops;

void *worker(void *arg) {
    int i;
    for (i = 0; i < loops; i++) {
	counter++;
    }
    return NULL;
}

int main(int argc, char *argv[]) {
    if (argc != 2) { 
	fprintf(stderr, "usage: threads <loops>\n"); 
	exit(1); 
    } 
    loops = atoi(argv[1]);
    pthread_t p1, p2;
    printf("Initial value : %d\n", counter);
    Pthread_create(&p1, NULL, worker, NULL); 
    Pthread_create(&p2, NULL, worker, NULL);
    Pthread_join(p1, NULL);
    Pthread_join(p2, NULL);
    printf("Final value   : %d\n", counter);
    return 0;
}
```

* Note that there's an included `common_threads.h` header, but this just defines some wrapper macros for some of the functions being used.
    * In particular, `Pthread_create()` and `Pthread_join()` are wrappers for the functions `pthread_create()` and `pthread_join()`, where the "p" is lowercase.
    * The details aren't very important for this example.

* This program creates two *threads*.
    * A thread can be thought of as a separate, but parallel, line of execution.
        * Parallel, here, should not be mistaken for *parallelism*.
            * Think of concurrency as a list of tasks that can be worked on where we can switch our attention between them as necessary.
            * Think of parallelism as having multiple workers doing these tasks simultaneously.
    * Multiple threads, then, allow for multiple lines of execution which may or may not converge at some point.

* These two threads work to add one to a counter some number of times, as specified by the arguments we pass to the program.
    * Two threads are adding 1 to a counter that starts at 0 a total of $N$ times each, where $N$ is the number we pass, and so we expect the output to be $2N$.

* It may not seem obvious at this point, but there is a bug in this program.
    * Let's look at a few executions.

```
shell> ./threads 100
Initial value : 0
Final value   : 200
shell> ./threads 1000
Initial value : 0
Final value   : 2000
shell> ./threads 10000
Initial value : 0
Final value   : 20000
```

* So far, so good, but what happens if we add one more 0 to the end?

```
shell> ./threads 100000
Initial value : 0
Final value   : 163963
```

* What happened here!?
    * Maybe it was a fluke?
    * Let's run a couple more times.

```
shell> ./threads 100000
Initial value : 0
Final value   : 198655
shell> ./threads 100000
Initial value : 0
Final value   : 200000
shell> ./threads 100000
Initial value : 0
Final value   : 188275
```

* Each time gives a different answer, and we even got lucky and saw the correct answer once.

* What's happening is that we're sharing a counter between the two threads.
    * The first few numbers we passed to the program were small enough for one thread to finish executing before the next started.
    * The last number, however, was large enough that there were context switches between the two threads mid-execution.
    * As a result, the two threads would end up seeing different values in the counter.
        * Suppose thread 1 saw a value of `12345` and the context switched to thread 2.
        * Thread 2 now sees `12345` in the counter and increments from there for awhile, and then we return to thread 1.
        * Thread 1 starts incrementing from `12345`, since that's the value it saw last, losing all of the progress that thread 2 had made.

* This is known as a *race condition* (or *race hazard*), specifically a *data race*, and is the result of our code not executing *atomically*.
    * This will be studied more in-depth when we get to the piece about concurrency.

## Persistence

* The third easy piece is *persistence*.
    * RAM is *volatile*, meaning a loss in power leads to a loss of any data stored in it.
    * *Persistent storage*, like hard drives or SSDs, don't lose the data stored with a loss in power.

* An operating system typically manages persistent storage using a *file system*.

* Unlike the CPU and memory, the abstraction provided by the OS for persistent storage doesn't create separate virtual disks for each program.
    * It is assumed that programs will want to share the data that gets stored in a file system.
    * In fact, the process of writing, compiling, and running a program requires that the data in the file system is shared between the editor, the compiler, the program, and the shell used to run all of the above.

File: `io.c`
```c
#include <stdio.h>
#include <unistd.h>
#include <assert.h>
#include <fcntl.h>
#include <sys/stat.h>
#include <sys/types.h>
#include <string.h>

int main(int argc, char *argv[]) {
    int fd = open("/tmp/file", O_WRONLY | O_CREAT | O_TRUNC, S_IRUSR | S_IWUSR);
    assert(fd >= 0);
    char buffer[20];
    sprintf(buffer, "hello world\n");
    int rc = write(fd, buffer, strlen(buffer));
    assert(rc == (strlen(buffer)));
    fsync(fd);
    close(fd);
    return 0;
}
```

* This program uses some interfaces provided by the OS to access and edit files without worrying about what the exact file system is.

* This program creates a file `/tmp/file` and writes "hello world" and a newline to it.

* This abstraction hides the nasty details that the operating system has to implement to make it work.
    * Depending on the storage medium, writing some bits isn't as easy as *just* writing the bits.
    * Finding the location where to read and/or write can be tough.
    * Even after finding the location, the OS still has to worry about maintaining the data structures used by the file system to keep track of everything and retain coherence between the logical files and the physical media that they're stored on.
    * And all of this is done behind the scenes while the user only has to see a few system calls.

* Beyond all of that, because writing to persistent storage is often orders of magnitude slower than to system memory, the OS often *batches* writes when possible.
    * As a result, techniques for keeping the internal state of the file system in working order after a crash are necessary.
        * A couple of these techniques are *journaling* and *copy-on-write*.

* When we reach the piece on persistence, we will see how these work in much more detail.

## OS Design Goals

* Now we know what an OS does:
    * It virtualizes physical resources, like the CPU, memory, and disks.
    * It handles a smooth, multi-application experience via concurrency.
    * It stores files persistently, making the information accessible over the long term.

* One tool that is used everywhere in computer science, and, of course, shows up here, is *abstraction*.
    * We use abstraction to turn large, specific problems into many smaller, and potentially more generic, problems.
    * Abstraction allows for a more modular design, where we can interchange parts depending on our needs and the evolution of technology.
    * This is because abstraction allows for interfaces between software and hardware that users don't have to worry about.

* Along with building the appropriate abstractions, we also have some other design goals that we can make progress towards to varying degrees:
    * Performance
    * Protection
    * Reliability
    * Energy-efficiency

* These design goals are all inter-related while also sometimes being incompatible to certain degrees.
    * For instance, an insecure system can't be said to be reliable or even performant when it's compromised.
    * Similarly, an unreliable system can't perform, and a system without any performance is quite unreliable.
    * On the other hand, many security features do introduce some performance overhead, as do many reliability features.
    * Energy-efficiency also shows up, as "too much" performance might eat into the energy budget.

* The first goal, performance, can be characterized mainly as minimizing the overhead of the OS.
    * Virtualization, while making it easier to work with the physical resources of the system, do add the overhead of calculating and of the necessary logic to make that virtualization work behind-the-scenes.
    * This means that we have some overhead in terms of instructions to compute address offsets, extra space usage due to file structures of the file system, extra memory usage from how memory is allocated, etc., and we want to do things in such a way that we can maintain other design goals without sacrificing too much performance.

* Protection, and to a further extent, *security*, is another important goal to strive for in developing an OS.
    * We want protection between applications so that they're not getting in each other's way; we do this by implementing some form of *isolation* between applications.
    * We want protection from external forces, such as malicious code that undermines the intended and/or desired use of a system by a user.

* The third design goal listed above, reliability, refers to a system's ability to avoid and correct, or otherwise gracefully handle, errors when running.
    * As systems grow more complex, so too does maintaining uptime.
    * Less uptime means less computing means less stuff getting done.

* Energy-efficiency is becoming much more relevant.
    * Probably the most dire existential crisis for humanity and other organisms on our planet is human-made (and exasperated) climate change due to the energy sources we use.
        * Minimizing energy usage of devices, hopefully without sacrificing performance, can help minimize the effects of climate change while energy generation moves to more clean and renewable sources (assuming we actually do it in time).
    * Beyond that, we also have mobile devices which are only convenient because they can run without being tied down to a single place all of the time.

## Some Historical Notes

* Operating Systems as we know them today are the result of many decades of work.

* The first "operating systems" were simple libraries and interfaces for abstracting away low-level operations so that programmers wouldn't have to reinvent the wheel every time they needed to compute something.

* As time went on, it became clear that some operations, such as file handling, should have some limits placed and be maintained by the operating system rather than giving the programmer unrestricted access to everything at all times.
    * These operations are implemented via system calls through special hardware instructions and states.
    * System calls transfers control to the OS, which has an elevated *hardware privilege level* that allows for looking at and changing sensitive data.
        * Internally, a program runs in what's called *user mode*, which restricts the sorts of operations it has access to.
        * A system call is used to elevate to *kernel mode*, which is initiated through something known as a *trap* or *interrupt* which gets transferred to a *trap handler*.
        * In kernel mode, the OS is able to access hardware more fully in ways that might be dangerous to allow a programmer to.
        * When the OS is done, it relinquishes control back to the user, while simultaneously de-escalating the privilege level, through a *return-from-trap* instruction.

* Eventually, computers would need to run more than one program at a time to best use all of their hardware, which became known as *multiprogramming*.
    * Because some programs would need to wait for system resources to respond with data from something like main memory or even a hard drive, other programs which were waiting to continue execution could start running during that time to utilize the CPU more efficiently.
    * However, because multiple programs would be running, protections needed to be in place, such as memory protection, to prevent one program from destroying another's work.
    * At the same time, the operating system, itself, would need to be written in a way to not run into concurrency issues, which we will see more of when we get to that piece.

* Nowadays, multiprogramming is essential, because we expect or computers to be multimedia devices that also do work for us.
    * In the early days of personal computers, a lot of historical lessons needed to be relearned.
        * Some early systems didn't seem to be designed in a way to provide inter-application protections or would lock up due to poorly implemented concurrency.
    * Since relearning lessons that were already history is much like reinventing the wheel, we should want to avoid it in production code.
        * Relearning the lessons on one's own time can be quite enlightening, though.

### Summary

* In this course, most of the topics above will be expanded on and explored to a decent level of depth.

* Unfortunately, topics such as networking or graphics devices *aren't* on the menu, currently.
    * However, since I (the writer of these notes) plan on taking some courses on [computer networking](https://omscs.gatech.edu/cs-6250-computer-networks) and [GPU software/hardware](https://omscs.gatech.edu/cs-8803-o21-gpu-hardware-and-software) through Georgia Tech's [OMSCS program](https://omscs.gatech.edu/), I might be able to fill in some of these gaps later on.

* As we move forward, hopefully any questions that the reader and/or I have will be answered!

**Next: Processes (Coming Soon)**